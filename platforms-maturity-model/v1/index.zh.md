* There is a centralized team or group responsible for tracking and managing the lifecycle of platform capabilities.
* A process exists for requesting new capabilities or enhancements to existing capabilities.
* There is a backlog of capabilities that need to be developed, maintained, or enhanced.
* Some level of prioritization exists for capabilities based on their value and impact.
* Upgrades and updates are planned and scheduled, but may still require manual intervention and coordination.
* There is a process for deprecating and retiring capabilities that are no longer needed or supported.

#### Example Scenarios:

* A centralized team maintains a roadmap of upcoming features and enhancements for platform capabilities.
* A process is in place for application teams to request new capabilities or enhancements to existing capabilities.
* Regular maintenance and upgrades are scheduled and communicated to users.

### Level 3, Scalable — Centrally enabled

Platforms and capabilities are planned, developed, and maintained in a coordinated and standardized manner. There is a well-defined process for managing the lifecycle of capabilities, including planning, development, testing, release, and ongoing maintenance. Platform teams work closely with product teams to understand their needs and prioritize the development of new capabilities and enhancements. Continuous integration and continuous delivery (CI/CD) practices are used to automate the deployment and release of capabilities.

#### Characteristics:

* There is a well-defined process for managing the lifecycle of platform capabilities, including planning, development, testing, release, and ongoing maintenance.
* Platform teams work closely with product teams to understand their needs and prioritize the development of new capabilities and enhancements.
* Continuous integration and continuous delivery (CI/CD) practices are used to automate the deployment and release of capabilities.
* Upgrades and updates are planned and executed in a standardized and automated manner.
* There is a process for monitoring and measuring the performance and reliability of platform capabilities.

#### Example Scenarios:

* Platform teams use agile methodologies to plan and prioritize the development of new capabilities and enhancements.
* CI/CD pipelines are used to automate the deployment and release of platform capabilities.
* Monitoring and alerting systems are in place to proactively identify and address issues with platform capabilities.

### Level 4, Optimizing — Managed services

Platforms and capabilities are managed as services, with dedicated teams responsible for their ongoing development, maintenance, and support. The teams follow best practices for software development and operations, including agile methodologies, continuous integration and continuous delivery (CI/CD), and site reliability engineering (SRE) principles. The teams have well-defined processes for managing the lifecycle of capabilities, including planning, development, testing, release, and ongoing maintenance. They use data-driven insights and feedback loops to continuously improve the performance, reliability, and user experience of platform capabilities.

#### Characteristics:

* Dedicated teams are responsible for the ongoing development, maintenance, and support of platform capabilities.
* Teams follow best practices for software development and operations, including agile methodologies, continuous integration and continuous delivery (CI/CD), and site reliability engineering (SRE) principles.
* There are well-defined processes for managing the lifecycle of capabilities, including planning, development, testing, release, and ongoing maintenance.
* Data-driven insights and feedback loops are used to continuously improve the performance, reliability, and user experience of platform capabilities.
* Platform teams actively seek feedback from users and use it to drive improvements and prioritize future development.

#### Example Scenarios:

* Platform teams use SRE principles to proactively monitor and address issues with platform capabilities.
* Continuous improvement processes are in place to regularly assess and enhance the performance and reliability of platform capabilities.
* User feedback is collected and used to drive improvements and prioritize future development.

{{< /tab >}}
{{< tab tabName="Measurement">}}

<h4 style="color:gray;padding-bottom:10px;padding-top:20px"><i>What is the process for gathering and incorporating feedback and learning?</i></h4>

Measurement is the process of gathering feedback and learning from users and stakeholders to continuously improve platform capabilities. It involves collecting data, analyzing it, and using the insights gained to make informed decisions and drive improvements. The measurement process should be consistent, systematic, and aligned with the goals and objectives of the platform.

### Level 1, Provisional — Ad hoc

Measurement is ad hoc and inconsistent. There is no formal process for gathering feedback or measuring the performance and impact of platform capabilities. Feedback is often anecdotal and based on individual experiences. There is little to no data collection or analysis, and no systematic approach to incorporating feedback into future development or improvement efforts.

#### Characteristics:

* Feedback is based on anecdotal experiences and individual opinions.
* There is no formal process for collecting or analyzing feedback.
* There is no systematic approach to incorporating feedback into future development or improvement efforts.
* There is little to no data collection or analysis.

#### Example Scenarios:

* Users provide feedback on platform capabilities through informal channels such as email or chat.
* Feedback is not tracked or documented, and there is no process for analyzing or incorporating it into future development efforts.

### Level 2, Operationalized — Consistent collection

Measurement is consistent and data-driven. There is a process for collecting feedback and measuring the performance and impact of platform capabilities. Feedback is collected through surveys, interviews, and other formal channels. Data is collected and analyzed to identify trends and patterns. The insights gained from the data are used to inform decision-making and drive improvements.

#### Characteristics:

* Feedback is collected through surveys, interviews, and other formal channels.
* Data is collected and analyzed to identify trends and patterns.
* The insights gained from the data are used to inform decision-making and drive improvements.
* There is a process for incorporating feedback into future development or improvement efforts.

#### Example Scenarios:

* Users are regularly surveyed to gather feedback on their experience with platform capabilities.
* Data is collected and analyzed to identify areas for improvement and prioritize future development efforts.
* Feedback is documented and tracked, and there is a process for incorporating it into future development or improvement efforts.

### Level 3, Scalable — Insights

Measurement is systematic and provides actionable insights. There is a well-defined process for gathering feedback and measuring the performance and impact of platform capabilities. Data is collected, analyzed, and used to generate insights that drive decision-making and improvements. The measurement process is integrated into the development and improvement lifecycle of platform capabilities.

#### Characteristics:

* There is a well-defined process for gathering feedback and measuring the performance and impact of platform capabilities.
* Data is collected, analyzed, and used to generate insights that drive decision-making and improvements.
* The measurement process is integrated into the development and improvement lifecycle of platform capabilities.
* Insights are used to inform prioritization, planning, and resource allocation for future development efforts.

#### Example Scenarios:

* Key performance indicators (KPIs) are defined and tracked to measure the performance and impact of platform capabilities.
* Data is collected and analyzed to identify areas for improvement and prioritize future development efforts.
* Insights are used to inform prioritization, planning, and resource allocation for future development efforts.

### Level 4, Optimizing — Quantitative and qualitative

Measurement is comprehensive and incorporates both quantitative and qualitative data. There is a holistic approach to gathering feedback and measuring the performance and impact of platform capabilities. Data is collected, analyzed, and used to generate insights that drive decision-making and improvements. The measurement process is continuous and iterative, with feedback and insights incorporated into every stage of the development and improvement lifecycle.

#### Characteristics:

* Measurement is comprehensive and incorporates both quantitative and qualitative data.
* Data is collected, analyzed, and used to generate insights that drive decision-making and improvements.
* The measurement process is continuous and iterative, with feedback and insights incorporated into every stage of the development and improvement lifecycle.
* Insights are used to inform prioritization, planning, and resource allocation for future development efforts.

#### Example Scenarios:

* A combination of quantitative metrics (e.g., user satisfaction scores, usage statistics) and qualitative feedback (e.g., user interviews, focus groups) is used to measure the performance and impact of platform capabilities.
* Data is collected, analyzed, and used to identify areas for improvement and prioritize future development efforts.
* Insights are used to inform prioritization, planning, and resource allocation for future development efforts.

{{< /tab >}}
{{< /tabs >}}
</div>

## Conclusion

This maturity model provides a framework for evaluating and improving platform engineering capabilities within an organization. By assessing the current state and identifying opportunities for improvement, organizations can strategically plan and invest in platform engineering to drive better outcomes and deliver more value to their users.

It is important to note that this model is not meant to be prescriptive or one-size-fits-all. Each organization is unique and may have different priorities, constraints, and goals. The model should be used as a starting point for reflection and discussion, and organizations should adapt and customize it to fit their specific needs and context.

By using this maturity model as a guide, organizations can navigate the complexities of platform engineering and accelerate their journey towards building and maintaining effective and impactful platforms.

## References

[Cloud Native Maturity Model](https://maturitymodel.cncf.io/)

[Platforms Definition White Paper](https://tag-app-delivery.cncf.io/whitepapers/platforms/)

* 应用团队根据紧迫需求创建新的功能。
* 中央团队提供组织内可共享服务的注册表。
* 对功能应用松散的标准，例如要求可自动化的API和使用文档。
* 使用基础设施即代码来实现部署服务的更容易追踪性。
* 通过服务清单启用符合性规定的审计，例如PCI DSS或HIPPA。
* 迁移和升级工作通过燃尽图进行跟踪，使组织能够跟踪符合性速率和完成时间。
* 跟踪并不表示支持水平；通常在这个阶段升级仍然是手动和定制的。

#### 示例场景：

* PostgreSQL 11将在年底停止支持。组织知道哪些数据库需要升级，并正在安排每个团队的工作以完成升级。

### Level 3, 可扩展 — 中央启用

平台和功能不仅被中央注册，还被中央协调。平台团队负责了解组织的广泛需求，并根据此优先处理平台和基础设施团队的工作。负责某个功能的人不仅要在技术上维护它，还要为将该功能与组织中的其他相关服务集成提供标准用户体验，确保安全可靠的使用，甚至提供可观察性。

存在创建和改进新功能的标准流程，使组织中的任何人都能贡献符合期望的解决方案。平台功能和特性的持续交付流程实现了定期的发布和回滚。对于像面向客户的产品更改一样，大型更改会进行计划和协调。

#### 特点：

* 应用团队在创建功能之前先向平台团队请求服务。
* 新服务必须遵循标准实践，例如标准接口、文档和治理。
* 升级过程在版本和服务之间是有文档和一致的。
* 在能力提供者不管理升级的情况下，他们为用户提供工具和支持，以减少影响。

#### 示例场景：

* 组织将升级到RHEL 9。为了启用此测试阶段，中央计算团队正在为每个团队设置具有正确软件和操作系统版本的测试环境。

### Level 4, 优化 — 托管服务

每个功能的生命周期都以标准化、自动化的方式进行管理。功能、特性和更新以连续的方式交付，对用户没有影响。由平台提供者发起的任何大型更改都包括为现有用户制定的迁移计划，其中定义了责任和时间表。

平台能力提供者承担了大部分维护责任，但有一个明确的合同——“共享责任模型”，描述了用户的责任，使双方能够基本上自主地运作。

#### 特点：

* 共享所有权模型明确定义了谁负责平台及其功能，以及用户的期望。
* 团队脚本化升级的执行和任何回滚策略，以降低风险和影响。

#### 示例场景：

* 虚拟机的用户不需要管理与版本升级相关的任何事务。他们唯一的要求是在交付流程中有一个包含代表性烟雾测试的阶段。然后，他们被要求声明他们的应用程序对风险的容忍度较低，以等待完全强化的升级，或对风险的容忍度较高，以成为早期采用者。然后，虚拟机能力会自动发布升级，包括在烟雾测试或金丝雀发布失败后的回滚。

#### 量化和定性的示例场景：

* 反馈和测量已深度融入组织的文化中。整个组织，从高层管理人员到全体工程师，都认识到数据收集和产品演进的反馈的价值。数据的民主化，包括平台用户和业务领导在内的各方利益相关者积极参与到平台改进的假设识别、设计过程中提供反馈，并在交付后测量影响。在规划平台举措时，所有这些测量都被考虑在内。

不仅利用标准框架，还理解从多个角度进行测量可以提供更全面的图片。关注识别领先指标，可以预测支持用户需求、减轻挑战并跟上行业趋势和业务需求的功能的出现。

#### 特点：

* 平台团队不断寻求改进他们关注的指标和数据收集方式。
* 组织熟悉并关注[古德哈特定律](https://en.wikipedia.org/wiki/Goodhart%27s_law)：“当一个度量成为目标时，它就不再是一个好的度量。”
* 对度量和遥测数据进行持续评估，以获取真正的洞察和价值。
* 有良好的度量数据管理支持，例如管理数据湖和推导洞察的标准平台能力。
* 鼓励跨部门合作，避免数据孤立，实现有效的反馈循环。

#### 示例场景：

* 随着时间的推移，组织收集到的数据显示构建时间增加了15%以上。这引发了开发人员的负面体验，一旦触发，即使构建时间降低到原始时间以下，开发人员的不满情绪也会持续较长时间。这一发现促使构建团队设定并遵守服务水平目标（SLO），在引发与用户的负面循环之前实现早期识别和改进。

{{< /tab >}}
{{< /tabs >}}
</div>

</br>

---
## 结论

平台及其维护者为敏捷数字产品开发提供了基础。它们提供了一系列一致的功能，使软件开发和交付更高效。这个成熟度模型为您的平台工程之旅提供了一张地图。