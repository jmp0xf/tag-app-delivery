* There is a centralized inventory of capabilities, including information on their owners, current state, and planned upgrades.
* Capability owners are responsible for maintaining and upgrading their capabilities, but there is no standardized process for doing so.
* There is a loosely defined process for requesting new capabilities or enhancements to existing capabilities.
* Capability owners may have some visibility into the needs and priorities of their users, but there is no formal feedback loop or mechanism for gathering user feedback.
* Upgrades and maintenance are planned and executed on an ad hoc basis, often in response to specific requests or incidents.

#### Example Scenarios:

* A centralized team maintains a catalog of available capabilities, including information on their owners, documentation, and support channels.
* Capability owners periodically review their capabilities and plan upgrades or enhancements based on user feedback or emerging requirements.

### Level 3, Scalable — Centrally enabled

Platforms and capabilities are centrally planned, prioritized, and developed based on user needs and organizational goals. There is a well-defined process for gathering user feedback and incorporating it into the development and maintenance of capabilities. Capability owners have a clear understanding of the needs and priorities of their users and are able to plan and execute upgrades and enhancements accordingly. There is a formal feedback loop between users and capability owners, allowing for continuous improvement and iteration.

#### Characteristics:

* There is a well-defined process for gathering user feedback and incorporating it into the development and maintenance of capabilities.
* Capability owners have a clear understanding of the needs and priorities of their users and are able to plan and execute upgrades and enhancements accordingly.
* There is a formal feedback loop between users and capability owners, allowing for continuous improvement and iteration.
* Upgrades and maintenance are planned and executed on a regular cadence, with minimal disruption to users.
* Capability owners have visibility into the overall platform roadmap and are able to align their plans and priorities with the broader organizational goals.

#### Example Scenarios:

* Capability owners regularly solicit user feedback through surveys, interviews, or user forums, and use that feedback to inform their development and maintenance plans.
* Upgrades and enhancements are planned and executed on a regular cadence, with clear communication to users about the timing and impact of the changes.

### Level 4, Optimizing — Managed services

Platforms and capabilities are managed as services, with dedicated teams responsible for their ongoing development, maintenance, and support. There is a well-defined process for gathering user feedback, incorporating it into the development and maintenance of capabilities, and measuring the impact of those changes. Capability owners have a deep understanding of the needs and priorities of their users and are able to proactively identify and address issues and opportunities for improvement. Upgrades and enhancements are planned and executed in a way that minimizes disruption to users and maximizes the value delivered.

#### Characteristics:

* Platforms and capabilities are managed as services, with dedicated teams responsible for their ongoing development, maintenance, and support.
* There is a well-defined process for gathering user feedback, incorporating it into the development and maintenance of capabilities, and measuring the impact of those changes.
* Capability owners have a deep understanding of the needs and priorities of their users and are able to proactively identify and address issues and opportunities for improvement.
* Upgrades and enhancements are planned and executed in a way that minimizes disruption to users and maximizes the value delivered.
* Capability owners have a clear understanding of the broader organizational goals and are able to align their plans and priorities accordingly.

#### Example Scenarios:

* Capability owners have dedicated teams responsible for the ongoing development, maintenance, and support of their capabilities.
* User feedback is regularly collected and used to inform the development and maintenance plans.
* Upgrades and enhancements are planned and executed in a way that minimizes disruption to users and maximizes the value delivered.

{{< /tab >}}
{{< tab tabName="Measurement">}}

<h4 style="color:gray;padding-bottom:10px;padding-top:20px"><i>What is the process for gathering and incorporating feedback and learning?</i></h4>

Measurement is the process of gathering feedback and learning from users and stakeholders to improve platforms and their capabilities. It involves collecting data, analyzing it, and using the insights gained to make informed decisions and drive continuous improvement. Measurement encompasses both quantitative data, such as usage metrics and performance indicators, and qualitative data, such as user feedback and satisfaction surveys.

### Level 1, Provisional — Ad hoc

Measurement is ad hoc and inconsistent. There is no formal process for gathering feedback or collecting data on platform usage and performance. User feedback is often anecdotal and based on individual experiences, and there is no systematic way to capture and analyze it. As a result, there is limited visibility into the effectiveness and impact of platform capabilities.

#### Characteristics:

* User feedback is ad hoc and based on individual experiences.
* There is no systematic way to capture and analyze user feedback.
* There is limited visibility into the effectiveness and impact of platform capabilities.
* There is no formal process for collecting data on platform usage and performance.

#### Example Scenarios:

* User feedback is shared informally through conversations or emails, but there is no centralized system for capturing and analyzing it.
* There is no data collection or analysis process in place to measure platform usage and performance.

### Level 2, Operationalized — Consistent collection

Measurement is a consistent and ongoing process, but the focus is primarily on collecting quantitative data, such as usage metrics and performance indicators. User feedback is collected through surveys or other feedback mechanisms, but there is limited analysis and action taken based on that feedback. There is some visibility into the effectiveness and impact of platform capabilities, but it is primarily based on quantitative data.

#### Characteristics:

* Quantitative data, such as usage metrics and performance indicators, is collected on a regular basis.
* User feedback is collected through surveys or other feedback mechanisms, but there is limited analysis and action taken based on that feedback.
* There is some visibility into the effectiveness and impact of platform capabilities, but it is primarily based on quantitative data.
* There is a consistent process for collecting and analyzing data, but it is primarily focused on quantitative metrics.

#### Example Scenarios:

* Usage metrics, such as number of requests or average response time, are collected and monitored on a regular basis.
* User satisfaction surveys are conducted periodically, but the feedback is not systematically analyzed or acted upon.

### Level 3, Scalable — Insights

Measurement is a well-defined and systematic process that encompasses both quantitative and qualitative data. User feedback is actively collected, analyzed, and used to drive continuous improvement. There is a deep understanding of the effectiveness and impact of platform capabilities, based on both quantitative and qualitative insights. Data-driven decision-making is a key part of the platform development and improvement process.

#### Characteristics:

* Both quantitative and qualitative data are collected and analyzed on a regular basis.
* User feedback is actively collected, analyzed, and used to drive continuous improvement.
* There is a deep understanding of the effectiveness and impact of platform capabilities, based on both quantitative and qualitative insights.
* Data-driven decision-making is a key part of the platform development and improvement process.

#### Example Scenarios:

* Usage metrics, user satisfaction surveys, and other quantitative data are collected and analyzed on a regular basis.
* User feedback is systematically collected, analyzed, and used to drive improvements to platform capabilities.
* Data-driven decision-making is a key part of the platform development and improvement process.

### Level 4, Optimizing — Quantitative and qualitative

Measurement is a comprehensive and integrated process that combines quantitative and qualitative data to provide a holistic view of platform performance and user satisfaction. Both quantitative and qualitative data are collected, analyzed, and used to drive continuous improvement. There is a culture of learning and experimentation, and data-driven decision-making is ingrained in the platform development and improvement process.

#### Characteristics:

* Both quantitative and qualitative data are collected, analyzed, and used to provide a holistic view of platform performance and user satisfaction.
* User feedback is systematically collected, analyzed, and used to drive continuous improvement.
* There is a culture of learning and experimentation, and data-driven decision-making is ingrained in the platform development and improvement process.

#### Example Scenarios:

* Usage metrics, user satisfaction surveys, user interviews, and other quantitative and qualitative data are collected and analyzed on a regular basis.
* User feedback is systematically collected, analyzed, and used to drive improvements to platform capabilities.
* Data-driven decision-making is ingrained in the platform development and improvement process, and there is a culture of learning and experimentation.

{{< /tab >}}
{{< /tabs >}}
</div>

## Conclusion

This maturity model provides a framework for evaluating and improving platform engineering capabilities within an organization. By assessing the current state of platform engineering and identifying opportunities for improvement, organizations can make informed decisions and take targeted actions to enhance their platform capabilities.

It is important to note that this model is not meant to be prescriptive or linear. Each organization is unique, and the specific path to maturity will vary. The model is intended to serve as a guide and a starting point for organizations seeking to improve their platform engineering practices.

By leveraging the insights and recommendations provided in this model, organizations can build and evolve their platform engineering discipline and create platforms that deliver value, efficiency, and innovation to their users and stakeholders.

For more information and resources, please refer to the [Platform Engineering Maturity Model](https://github.com/cncf/tag-app-delivery/raw/main/platforms-maturity-model/v1/assets/platform-eng-maturity-model-v1.0.pdf).

## References

- [Cloud Maturity Model](https://maturitymodel.cncf.io/)
- [Platforms Definition White Paper](https://tag-app-delivery.cncf.io/whitepapers/platforms/)

## About CNCF

The Cloud Native Computing Foundation (CNCF) hosts critical components of the global technology infrastructure. CNCF brings together the world's top developers, end users, and vendors and runs the largest open-source developer conferences. CNCF is part of the nonprofit Linux Foundation.

Learn more about CNCF and its projects at [cncf.io](https://www.cncf.io/).

## About the Authors

This document was created by the CNCF TAG App Delivery Working Group. The authors of this document are:

- [Author 1]
- [Author 2]
- [Author 3]

For a complete list of contributors, please refer to the [GitHub repository](https://github.com/cncf/tag-app-delivery/tree/main/platforms-maturity-model).

## License

This document is licensed under the Creative Commons Attribution 4.0 International License. To view a copy of this license, visit [http://creativecommons.org/licenses/by/4.0/](http://creativecommons.org/licenses/by/4.0/).

* 应用团队根据紧迫需求创建新的功能。
* 中央团队提供组织内可共享服务的注册表。
* 对功能应用松散的标准，例如要求可自动化的API和使用文档。
* 使用基础设施即代码来实现部署服务的更容易追踪性。
* 通过服务清单启用符合性监察，例如PCI DSS或HIPPA。
* 迁移和升级工作通过燃尽图进行跟踪，使组织能够跟踪符合性速率和完成时间。
* 跟踪不表示支持水平；通常在这个阶段升级仍然是手动和定制的。

#### 示例场景：

* PostgreSQL 11将在年底停止支持。组织知道哪些数据库需要升级，并计划在每个团队的待办事项中完成工作。

### Level 3, 可扩展 — 中央启用

平台和功能不仅在中央注册，而且在中央协调。平台团队负责了解组织的广泛需求，并相应地优先处理平台和基础设施团队的工作。负责某个功能的人不仅要在技术上维护它，还要为将该功能与组织中的其他相关服务集成提供标准用户体验，确保安全可靠的使用，甚至提供可观察性。

存在创建和发展新功能的标准流程，使组织中的任何人都能贡献符合期望的解决方案。平台功能和特性的持续交付流程实现了定期的发布和回滚。大型变更计划和协调与面向客户的产品变更相同。

#### 特点：

* 应用团队在创建功能之前首先向平台团队请求服务。
* 新服务必须遵循标准实践，例如标准接口、文档和治理。
* 升级过程在版本和服务之间是有文档和一致的。
* 如果能力提供者不管理升级，他们会为用户提供工具和支持，以减少影响。

#### 示例场景：

* 组织将升级到RHEL 9。为了启用测试阶段，每个应用团队都需要验证其软件是否继续正常工作。为此，中央计算团队正在为每个团队设置具有正确软件和操作系统版本的测试环境。

### Level 4, 优化 — 托管服务

每个功能的生命周期都以标准化、自动化的方式进行管理。功能、特性和更新以连续的方式交付，不对用户产生影响。由平台提供者发起的任何大型变更都包括为现有用户制定的迁移计划，其中定义了责任和时间表。

平台能力提供者承担了大部分维护责任，但有一个明确的合同——“共享责任模型”，描述了用户的责任，使双方能够基本上自主地运作。

#### 特点：

* 共享所有权模型明确定义了平台及其能力的责任和用户的期望。
* 团队脚本化升级的执行和任何回滚策略，以降低风险和影响。

#### 示例场景：

* 虚拟机的用户不需要管理与版本升级相关的任何事务。他们唯一的要求是在交付流程中包含一个代表性的冒烟测试阶段。然后，他们被要求将他们的应用程序声明为对完全强化的升级具有较低的风险容忍度，或对成为早期采用者具有较高的容忍度。然后，虚拟机能力会自动发布升级，包括在冒烟测试或金丝雀发布失败后的回滚。

#### 例子：

* 随着时间的推移，组织收集到的数据显示构建时间增加了15%以上。这引发了开发人员的负面体验，一旦触发，即使构建时间降低到原始时间以下，开发人员的不满情绪也会持续较长时间。这一发现促使构建团队设定并遵守服务水平目标（SLO），在引发与用户的负面循环之前实现早期识别和改进。